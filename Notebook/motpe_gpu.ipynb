{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from pygmo import hypervolume # for computing hypervolume indicator\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GammaFunction:\n",
    "    \"\"\"\n",
    "    Gamma (parameter) function for the MotPE sampler.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=0.10):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        return number of bad samples\n",
    "        '''\n",
    "        return int(lax.floor(self.gamma * x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPESampler\n",
    "An implmentation of the Tree Parzen Estimator Sampler\n",
    "\n",
    "### NDSort\n",
    "The goal of NDsort is to assign nondomination rank to each of objective funciton results (y value).\n",
    "We rank by strongly domination $$y \\in \\mathbb{R}^m $$\n",
    "Here I provide an example on how NDSort works:\n",
    "- Assume we have $\\mathbb{R} = 2$ on objective function space, then given each $y_sample$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPESampler:\n",
    "    def __init__(self,\n",
    "                 hyper_param,  # Hyperparameters\n",
    "                 observations,  # Observations of the objective function\n",
    "                 random_state,  # Random seed\n",
    "                 n_EI_candidates=24,  # Number of candidates for EI maximization\n",
    "                 rule='james',  # Rule to use for the next best hyperparameter to sample\n",
    "                 gamma_func=GammaFunction(),  # A function to compute the gamma parameter\n",
    "                 weights_func=default_weights,  # A function to compute the weights\n",
    "                 split_cache=None):  # A cache of the splits\n",
    "        self.hyper_param_ = hyper_param\n",
    "        self.observations_ = observations\n",
    "        self.random_state_ = random_state\n",
    "        self.n_EI_candidates_ = n_EI_candidates\n",
    "        self.rule_ = rule\n",
    "        self.gamma_func_ = gamma_func  # split gamma\n",
    "        self.weights_func_ = weights_func\n",
    "\n",
    "        # this is for split observation\n",
    "        if split_cache is None:\n",
    "            self.split_cache_ = {}\n",
    "        else:\n",
    "            self.split_cache_ = split_cache.copy()\n",
    "\n",
    "    def NDSort(self, y_val):\n",
    "        '''\n",
    "        Non-dominated rank\n",
    "        '''\n",
    "        tmp_y_val = y_val.copy()\n",
    "        NDRanks = jnp.zeros(len(y_val))\n",
    "\n",
    "        current_rank = 0\n",
    "        cnt = len(y_val)\n",
    "\n",
    "        while cnt > 0:\n",
    "            # change into 3D array with y_val.shape[0] along axis 0\n",
    "            y_val_extend = jnp.tile(tmp_y_val, reps=(y_val.shape[0], 1, 1))\n",
    "            y_val_swap = jnp.swapaxes(y_val_extend, 0, 1)\n",
    "            dominance = jnp.sum(\n",
    "                jnp.all(y_val_extend < y_val_swap, axis=2), axis=1)\n",
    "            # assigned entry will be set to inf\n",
    "            tmp_y_val.at[dominance == 0].set(jnp.finfo(jnp.float32).max)\n",
    "            NDRanks.at[dominance == 0].set(current_rank)\n",
    "            current_rank += 1\n",
    "            cnt -= jnp.sum(dominance == 0)\n",
    "\n",
    "        return NDRanks\n",
    "\n",
    "    def split_observation(self, y_collection, D_l_sample_cnt):\n",
    "        '''\n",
    "        Algorithm 2 in Journal\n",
    "        Split the observations into good and bad observations\n",
    "        '''\n",
    "        # TODO: how to cache results?\n",
    "        # I dont know if this will work. Maybe use pickle is a better idea\n",
    "        cache_key = y_collection.tobytes()\n",
    "\n",
    "        if cache_key in self.split_cache_:\n",
    "            # bad samples indices\n",
    "            D_l_sample_indices = self.split_cache_[cache_key][\"l_x_i\"]\n",
    "            # good samples indices\n",
    "            D_g_sample_indices = self.split_cache_[cache_key][\"g_x_i\"]\n",
    "        else:\n",
    "            y_ranks = self.NDSort(y_collection)\n",
    "            indices = jnp.array(range(len(y_collection)))\n",
    "            D_l_sample_cnt = jnp.array(range(len(y_ranks)))\n",
    "            D_l_sample_indices = jnp.array([], dtype=jnp.int32)\n",
    "\n",
    "            # gather D_l_sample until the D_l_sample_cnt is reached\n",
    "            # step 1: gather until D_l_sample_cnt is almost reached\n",
    "            rank = 0\n",
    "            while (len(D_l_sample_indices) + jnp.sum(y_ranks == rank)) <= D_l_sample_cnt:\n",
    "                D_l_sample_indices = jnp.concatenate(\n",
    "                    (D_l_sample_indices, indices[y_ranks == 0]))\n",
    "                rank += 1\n",
    "\n",
    "            # HSSP\n",
    "            # step 2: gather subset until HSSP is reached\n",
    "            y_refer_pt = y_collection[y_ranks == rank]\n",
    "            y_refer_pt_indices = indices[y_ranks == rank]\n",
    "            y_worst = jnp.max(y_collection, axis=0)\n",
    "\n",
    "            # reference pointer creation: lower bound by eps\n",
    "            reference_pt = jnp.maximum(\n",
    "                1.1 * y_worst, jnp.full(y_worst.shape[1], jnp.finfo.eps))\n",
    "\n",
    "            # collecting subsets\n",
    "            # TODO, better name\n",
    "            y_subset = []\n",
    "            HV_constribution = []\n",
    "\n",
    "            # Algorithm 3 in Journal, line 2-3\n",
    "            for j in range(len(y_refer_pt)):\n",
    "                HV_indicator = hypervolume(\n",
    "                    [y_refer_pt[j]]).compute(reference_pt)\n",
    "                HV_constribution.append(HV_indicator)\n",
    "\n",
    "            # fill the rest of the D_l_sample_indices\n",
    "            while len(D_l_sample_indices) + 1 < D_l_sample_cnt:\n",
    "                HV_subset_indicator = 0\n",
    "                if len(y_subset) > 0:\n",
    "                    HV_subset_indicator = hypervolume(\n",
    "                        y_subset).compute(reference_pt)\n",
    "                max_contribution_indices = jnp.argmax(HV_constribution)\n",
    "\n",
    "                # assigned points will be set to -inf to avoid being selected again\n",
    "                HV_constribution[max_contribution_indices] = -1 * jnp.inf\n",
    "\n",
    "                for i in range(len(HV_constribution)):\n",
    "                    # skip current max\n",
    "                    if i == max_contribution_indices:\n",
    "                        continue\n",
    "\n",
    "                    HV_constribution[i] = HV_constribution[i] - (hypervolume(y_subset + [jnp.max(y_refer_pt[max_contribution_indices], y_refer_pt[i], axis=0)]).compute(reference_pt) - HV_subset_indicator)\n",
    "                \n",
    "                y_subset += y_refer_pt[max_contribution_indices]\n",
    "                D_l_sample_indices = jnp.concatenate(\n",
    "                    (D_l_sample_indices, y_refer_pt_indices[max_contribution_indices]))\n",
    "                \n",
    "            # step 3: gather the rest of the points as D_g_sample_indices\n",
    "            D_g_sample_indices = jnp.setdiff1d(indices, D_l_sample_indices)\n",
    "\n",
    "            # cache the results\n",
    "            self.split_cache_[cache_key] = {\n",
    "                \"l_x_i\": D_l_sample_indices,\n",
    "                \"g_x_i\": D_g_sample_indices\n",
    "            }\n",
    "\n",
    "        return D_l_sample_indices, D_g_sample_indices\n",
    "    \n",
    "\n",
    "    def get_type(self):\n",
    "        '''\n",
    "        This is only compatible with configspace\n",
    "        TODO need to change\n",
    "        '''\n",
    "        cs_dist = str(type(self.hp))\n",
    "        if 'Integer' in cs_dist:\n",
    "            return int\n",
    "        elif 'Float' in cs_dist:\n",
    "            return float\n",
    "        elif 'Categorical' in cs_dist:\n",
    "            var_type = type(self.hp.choices[0])\n",
    "            if var_type == str or var_type == bool:\n",
    "                return var_type\n",
    "            else:\n",
    "                raise ValueError('The type of categorical parameters must be \"bool\" or \"str\".')\n",
    "        else:\n",
    "            raise NotImplementedError('The distribution is not implemented.')\n",
    "\n",
    "    def sample(self):\n",
    "        # Load the observed hyperparameter values and corresponding function values\n",
    "        hyper_param, y_collection=self.load_hyper_param_and_y_collection()\n",
    "\n",
    "        # use gamma to determine the split ratio\n",
    "        D_l_cnt = self.gamma_func_(len(hyper_param))\n",
    "        D_l_sample_indices, D_g_sample_indices = self.split_observation(\n",
    "            y_collection, D_l_cnt)\n",
    "        D_l_sample = hyper_param[D_l_sample_indices]\n",
    "        D_g_sample = hyper_param[D_g_sample_indices]\n",
    "\n",
    "        # Determine the type of the hyperparameter\n",
    "        var_type = self.get_type()\n",
    "\n",
    "        # Sample a new hyperparameter\n",
    "        if var_type in [float, int]:\n",
    "            new_hyper_param = self.sample_continuous(D_l_sample, D_g_sample)\n",
    "        elif var_type == str:\n",
    "            new_hyper_param = self.sample_categorical(D_l_sample, D_g_sample)\n",
    "        \n",
    "        new_hyper_param = self.revert_h(new_hyper_param)\n",
    "        return new_hyper_param\n",
    "    \n",
    "    def sample_continuous(self, D_l_sample, D_g_sample):\n",
    "        pass\n",
    "\n",
    "    def sample_categorical(self, D_l_sample, D_g_sample):\n",
    "        pass\n",
    "\n",
    "    def revert_h(self, hyper_param_val):\n",
    "        pass\n",
    "\n",
    "    def load_hyper_param_and_y_collection(self):\n",
    "        # Load the hyperparameter values\n",
    "        hyper_param=[]\n",
    "        y_collection=[]\n",
    "\n",
    "        # TODO: this data structure can be vectorized\n",
    "        for obs in self.observations_:\n",
    "            if self.hyper_param_.name in obs['params']:\n",
    "                # TODO convert_h\n",
    "                hyper_param.append(self.convert_h(\n",
    "                    obs['params'][self.hyper_param_.name]))\n",
    "                y_collection.append(obs['f_value'].values())\n",
    "\n",
    "        jnp.asarray(hyper_param)\n",
    "        jnp.asarray(y_collection, axis=0)\n",
    "        return hyper_param, y_collection\n",
    "\n",
    "    # TODO: better name?\n",
    "    def convert_h(self, hyper_param_val):\n",
    "        try:\n",
    "            lower_bound, upper_bound, _=self.get_bound_and_q()\n",
    "            if self.hyper_param_.type == 'loguniform':\n",
    "                hyper_param_val=lax.log(hyper_param_val)\n",
    "            return (hyper_param_val - lower_bound) / (upper_bound - lower_bound)\n",
    "        except NotImplementedError:\n",
    "            raise NotImplementedError(\n",
    "                'Categorical parameters do not have lower and upper options.')\n",
    "\n",
    "    def get_bound_and_q(self):\n",
    "        try:\n",
    "            if self.hyper_param_.type == 'loguniform':\n",
    "                return lax.log(self.hyper_param_.lower), lax.log(self.hyper_param_.upper), self.hyper_param_.q\n",
    "            else:\n",
    "                return self.hyper_param_.lower, self.hyper_param_.upper, self.hyper_param_.q\n",
    "        except NotImplementedError:\n",
    "            raise NotImplementedError(\n",
    "                'Categorical parameters do not have the log scale option.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motpe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
