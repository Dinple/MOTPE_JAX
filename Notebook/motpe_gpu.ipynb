{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from jax import random\n",
    "from pygmo import hypervolume # for computing hypervolume indicator\n",
    "import kernal_util as kernal\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GammaFunction:\n",
    "    \"\"\"\n",
    "    Gamma (parameter) function for the MotPE sampler.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=0.10):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        return number of bad samples\n",
    "        '''\n",
    "        return int(lax.floor(self.gamma * x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPESampler\n",
    "An implmentation of the Tree Parzen Estimator Sampler\n",
    "\n",
    "### NDSort\n",
    "The goal of NDsort is to assign nondomination rank to each of objective funciton results (y value).\n",
    "We rank by strongly domination $$y \\in \\mathbb{R}^m $$\n",
    "Here I provide an example on how NDSort works:\n",
    "- Assume we have $\\mathbb{R} = 2$ on objective function space, then given each $y_sample$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPESampler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hyper_param,  # Hyperparameters\n",
    "        observations,  # Observations of the objective function\n",
    "        random_state,  # Random seed\n",
    "        n_EI_candidates=24,  # Number of candidates for EI maximization\n",
    "        rule=\"james\",  # Rule to use for the next best hyperparameter to sample\n",
    "        gamma_func=GammaFunction(),  # A function to compute the gamma parameter\n",
    "        weights_func=default_weights,  # A function to compute the weights\n",
    "        split_cache=None,\n",
    "    ):  # A cache of the splits\n",
    "        self.hyper_param_ = hyper_param\n",
    "        self.observations_ = observations\n",
    "        self.random_state_ = random_state\n",
    "        self.n_EI_candidates_ = n_EI_candidates\n",
    "        self.rule_ = rule\n",
    "        self.gamma_func_ = gamma_func  # split gamma\n",
    "        self.weights_func_ = weights_func\n",
    "\n",
    "        # this is for split observation\n",
    "        if split_cache is None:\n",
    "            self.split_cache_ = {}\n",
    "        else:\n",
    "            self.split_cache_ = split_cache.copy()\n",
    "\n",
    "    def NDSort(self, y_val):\n",
    "        \"\"\"\n",
    "        Non-dominated rank\n",
    "        \"\"\"\n",
    "        tmp_y_val = y_val.copy()\n",
    "        NDRanks = jnp.zeros(len(y_val))\n",
    "\n",
    "        current_rank = 0\n",
    "        cnt = len(y_val)\n",
    "\n",
    "        while cnt > 0:\n",
    "            # change into 3D array with y_val.shape[0] along axis 0\n",
    "            y_val_extend = jnp.tile(tmp_y_val, reps=(y_val.shape[0], 1, 1))\n",
    "            y_val_swap = jnp.swapaxes(y_val_extend, 0, 1)\n",
    "            dominance = jnp.sum(jnp.all(y_val_extend < y_val_swap, axis=2), axis=1)\n",
    "            # assigned entry will be set to inf\n",
    "            tmp_y_val.at[dominance == 0].set(jnp.finfo(jnp.float32).max)\n",
    "            NDRanks.at[dominance == 0].set(current_rank)\n",
    "            current_rank += 1\n",
    "            cnt -= jnp.sum(dominance == 0)\n",
    "\n",
    "        return NDRanks\n",
    "\n",
    "    def split_observation(self, y_collection, D_l_sample_cnt):\n",
    "        \"\"\"\n",
    "        Algorithm 2 in Journal\n",
    "        Split the observations into good and bad observations\n",
    "        \"\"\"\n",
    "        # TODO: how to cache results?\n",
    "        # I dont know if this will work. Maybe use pickle is a better idea\n",
    "        cache_key = y_collection.tobytes()\n",
    "\n",
    "        if cache_key in self.split_cache_:\n",
    "            # bad samples indices\n",
    "            D_l_sample_indices = self.split_cache_[cache_key][\"l_x_i\"]\n",
    "            # good samples indices\n",
    "            D_g_sample_indices = self.split_cache_[cache_key][\"g_x_i\"]\n",
    "        else:\n",
    "            y_ranks = self.NDSort(y_collection)\n",
    "            indices = jnp.array(range(len(y_collection)))\n",
    "            D_l_sample_cnt = jnp.array(range(len(y_ranks)))\n",
    "            D_l_sample_indices = jnp.array([], dtype=jnp.int32)\n",
    "\n",
    "            # gather D_l_sample until the D_l_sample_cnt is reached\n",
    "            # step 1: gather until D_l_sample_cnt is almost reached\n",
    "            rank = 0\n",
    "            while (\n",
    "                len(D_l_sample_indices) + jnp.sum(y_ranks == rank)\n",
    "            ) <= D_l_sample_cnt:\n",
    "                D_l_sample_indices = jnp.concatenate(\n",
    "                    (D_l_sample_indices, indices[y_ranks == 0])\n",
    "                )\n",
    "                rank += 1\n",
    "\n",
    "            # HSSP\n",
    "            # step 2: gather subset until HSSP is reached\n",
    "            y_curr_rank_sample = y_collection[y_ranks == rank]\n",
    "            y_curr_rank_indices = indices[y_ranks == rank]\n",
    "            y_worst = jnp.max(y_collection, axis=0)\n",
    "\n",
    "            # reference pointer creation: lower bound by eps\n",
    "            # TODO: here we assume positivity of the objective function\n",
    "            esp_pad = jnp.full(y_worst.shape[1], jnp.finfo.eps)\n",
    "            # multiply by 1.1 to ensure that the reference point is worse than the worst point\n",
    "            # esp_pad to ensure the minimum value is not 0 or negative\n",
    "            reference_pt = jnp.maximum(1.1 * y_worst, esp_pad)\n",
    "\n",
    "            # collecting subsets\n",
    "            # TODO, better name\n",
    "            y_subset = []           # D_s in the Journal\n",
    "            HV_constrib = []   # c_y in the Journal\n",
    "\n",
    "            # Algorithm 3 in Journal, line 2-3\n",
    "            for j in range(len(y_curr_rank_sample)):\n",
    "                HV_indicator = hypervolume([y_curr_rank_sample[j]]).compute(\n",
    "                    reference_pt\n",
    "                )\n",
    "                HV_constrib.append(HV_indicator)\n",
    "\n",
    "            # fill the rest of the D_l_sample_indices, line 5-12\n",
    "            while len(D_l_sample_indices) + 1 < D_l_sample_cnt:\n",
    "                HV_subset_indicator = 0\n",
    "                if len(y_subset) > 0:\n",
    "                    HV_subset_indicator = hypervolume(y_subset).compute(reference_pt)\n",
    "                max_contrib_indices = jnp.argmax(HV_constrib)\n",
    "\n",
    "                # assigned points will be set to -inf to avoid being selected again\n",
    "                HV_constrib[max_contrib_indices] = -1 * jnp.inf\n",
    "\n",
    "                for i in range(len(HV_constrib)):\n",
    "                    # skip current max\n",
    "                    if i == max_contrib_indices:\n",
    "                        continue\n",
    "                    \n",
    "                    HV_constrib[i] = HV_constrib[i] - (\n",
    "                        hypervolume(\n",
    "                            y_subset\n",
    "                            + [\n",
    "                                jnp.max(\n",
    "                                    [y_curr_rank_sample[max_contrib_indices],\n",
    "                                    y_curr_rank_sample[i]],\n",
    "                                    axis=0,\n",
    "                                )\n",
    "                            ]\n",
    "                        ).compute(reference_pt)\n",
    "                        - HV_subset_indicator\n",
    "                    )\n",
    "\n",
    "                y_subset += y_curr_rank_sample[max_contrib_indices]\n",
    "                D_l_sample_indices = jnp.concatenate(\n",
    "                    (D_l_sample_indices, y_curr_rank_indices[max_contrib_indices])\n",
    "                )\n",
    "\n",
    "            # step 3: gather the rest of the points as D_g_sample_indices\n",
    "            D_g_sample_indices = jnp.setdiff1d(indices, D_l_sample_indices)\n",
    "\n",
    "            # cache the results\n",
    "            self.split_cache_[cache_key] = {\n",
    "                \"l_x_i\": D_l_sample_indices,\n",
    "                \"g_x_i\": D_g_sample_indices,\n",
    "            }\n",
    "\n",
    "        return D_l_sample_indices, D_g_sample_indices\n",
    "\n",
    "    def get_type(self):\n",
    "        \"\"\"\n",
    "        This is only compatible with configspace\n",
    "        TODO need to change\n",
    "        \"\"\"\n",
    "        cs_dist = str(type(self.hp))\n",
    "        if \"Integer\" in cs_dist:\n",
    "            return int\n",
    "        elif \"Float\" in cs_dist:\n",
    "            return float\n",
    "        elif \"Categorical\" in cs_dist:\n",
    "            var_type = type(self.hp.choices[0])\n",
    "            if var_type == str or var_type == bool:\n",
    "                return var_type\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    'The type of categorical parameters must be \"bool\" or \"str\".'\n",
    "                )\n",
    "        else:\n",
    "            raise NotImplementedError(\"The distribution is not implemented.\")\n",
    "\n",
    "    def sample(self):\n",
    "        # Load the observed hyperparameter values and corresponding function values\n",
    "        hyper_param, y_collection = self.load_hyper_param_and_y_collection()\n",
    "\n",
    "        # use gamma to determine the split ratio\n",
    "        D_l_cnt = self.gamma_func_(len(hyper_param))\n",
    "        D_l_sample_indices, D_g_sample_indices = self.split_observation(\n",
    "            y_collection, D_l_cnt\n",
    "        )\n",
    "        D_l_sample = hyper_param[D_l_sample_indices]\n",
    "        D_g_sample = hyper_param[D_g_sample_indices]\n",
    "\n",
    "        # Determine the type of the hyperparameter\n",
    "        var_type = self.get_type()\n",
    "\n",
    "        # Sample a new hyperparameter\n",
    "        if var_type in [float, int]:\n",
    "            new_hyper_param = self.sample_continuous(D_l_sample, D_g_sample, var_type)\n",
    "        elif var_type == str:\n",
    "            new_hyper_param = self.sample_categorical(D_l_sample, D_g_sample, var_type)\n",
    "\n",
    "        new_hyper_param = self.revert_h(new_hyper_param)\n",
    "        return new_hyper_param\n",
    "\n",
    "    def sample_continuous(self, D_l_sample, D_g_sample, var_type):\n",
    "        pass\n",
    "\n",
    "    def sample_categorical(self, D_l_sample, D_g_sample, var_type):\n",
    "        pass\n",
    "\n",
    "    def revert_h(self, hyper_param_val):\n",
    "        pass\n",
    "\n",
    "    def load_hyper_param_and_y_collection(self):\n",
    "        # Load the hyperparameter values\n",
    "        hyper_param = []\n",
    "        y_collection = []\n",
    "\n",
    "        # TODO: this data structure can be vectorized\n",
    "        for obs in self.observations_:\n",
    "            if self.hyper_param_.name in obs[\"params\"]:\n",
    "                # TODO convert_h\n",
    "                hyper_param.append(\n",
    "                    self.convert_h(obs[\"params\"][self.hyper_param_.name])\n",
    "                )\n",
    "                y_collection.append(obs[\"f_value\"].values())\n",
    "\n",
    "        jnp.asarray(hyper_param)\n",
    "        jnp.asarray(y_collection, axis=0)\n",
    "        return hyper_param, y_collection\n",
    "\n",
    "    # TODO: better name?\n",
    "    def convert_h(self, hyper_param_val):\n",
    "        try:\n",
    "            lower_bound, upper_bound, _ = self.get_bound_and_q()\n",
    "            if self.hyper_param_.type == \"loguniform\":\n",
    "                hyper_param_val = lax.log(hyper_param_val)\n",
    "            return (hyper_param_val - lower_bound) / (upper_bound - lower_bound)\n",
    "        except NotImplementedError:\n",
    "            raise NotImplementedError(\n",
    "                \"Categorical parameters do not have lower and upper options.\"\n",
    "            )\n",
    "\n",
    "    def get_bound_and_q(self):\n",
    "        try:\n",
    "            if self.hyper_param_.type == \"loguniform\":\n",
    "                return (\n",
    "                    lax.log(self.hyper_param_.lower),\n",
    "                    lax.log(self.hyper_param_.upper),\n",
    "                    self.hyper_param_.q,\n",
    "                )\n",
    "            else:\n",
    "                return (\n",
    "                    self.hyper_param_.lower,\n",
    "                    self.hyper_param_.upper,\n",
    "                    self.hyper_param_.q,\n",
    "                )\n",
    "        except NotImplementedError:\n",
    "            raise NotImplementedError(\n",
    "                \"Categorical parameters do not have the log scale option.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinousParzenEstimator:\n",
    "    def __init__(self, lower, upper, q, prior_weight=1.0, consider_prior=True):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalParzenEstimator:\n",
    "    def __init__(self, choices, prior_weight=1.0, consider_prior=True):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOTPE:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motpe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
