{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GammaFunction:\n",
    "    \"\"\"\n",
    "    Gamma (parameter) function for the MotPE sampler.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=0.10):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        return number of bad samples\n",
    "        '''\n",
    "        return int(lax.floor(self.gamma * x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPESampler\n",
    "An implmentation of the Tree Parzen Estimator Sampler\n",
    "\n",
    "### NDSort\n",
    "The goal of NDsort is to assign nondomination rank to each of objective funciton results (y value).\n",
    "We rank by strongly domination $$y \\in \\mathbb{R}^m $$\n",
    "Here I provide an example on how NDSort works:\n",
    "- Assume we have $\\mathbb{R} = 2$ on objective function space, then given each $y_sample$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPESampler:\n",
    "    def __init__(self,\n",
    "                 hyper_param,  # Hyperparameters\n",
    "                 observations,  # Observations of the objective function\n",
    "                 random_state,  # Random seed\n",
    "                 n_EI_candidates=24,  # Number of candidates for EI maximization\n",
    "                 rule='james',  # Rule to use for the next best hyperparameter to sample\n",
    "                 gamma_func=GammaFunction(),  # A function to compute the gamma parameter\n",
    "                 weights_func=default_weights,  # A function to compute the weights\n",
    "                 split_cache=None):  # A cache of the splits\n",
    "        self.hyper_param_ = hyper_param\n",
    "        self.observations_ = observations\n",
    "        self.random_state_ = random_state\n",
    "        self.n_EI_candidates_ = n_EI_candidates\n",
    "        self.rule_ = rule\n",
    "        self.gamma_func_ = gamma_func # split gamma\n",
    "        self.weights_func_ = weights_func\n",
    "        \n",
    "        # this is for split observation\n",
    "        if split_cache is None:\n",
    "            self.split_cache_ = {}\n",
    "        else:\n",
    "            self.split_cache_ = split_cache.copy()\n",
    "\n",
    "    def NDSort(self, y_val):\n",
    "        '''\n",
    "        Non-dominated rank\n",
    "        '''\n",
    "        tmp_y_val = y_val.copy()\n",
    "        NDRanks = jnp.zeros(len(y_val))\n",
    "\n",
    "        current_rank = 0\n",
    "        cnt = len(y_val)\n",
    "\n",
    "        while cnt > 0:\n",
    "            # change into 3D array with y_val.shape[0] along axis 0\n",
    "            y_val_extend = jnp.tile(tmp_y_val, reps=(y_val.shape[0], 1, 1))\n",
    "            y_val_swap = jnp.swapaxes(y_val_extend, 0, 1)\n",
    "            dominance = jnp.sum(jnp.all(y_val_extend < y_val_swap, axis=2), axis=1)\n",
    "            # assigned entry will be set to inf\n",
    "            tmp_y_val.at[dominance == 0].set(jnp.finfo(jnp.float32).max)\n",
    "            NDRanks.at[dominance == 0].set(current_rank)\n",
    "            current_rank += 1\n",
    "            cnt -= jnp.sum(dominance == 0)\n",
    "        \n",
    "        return NDRanks\n",
    "\n",
    "    \n",
    "    def split_observation(self, y_collection, num_bad_samples):\n",
    "        '''\n",
    "        Algorithm 2 in Journal\n",
    "        Split the observations into good and bad observations\n",
    "        '''\n",
    "        # TODO: how to cache results?\n",
    "        # I dont know if this will work. Maybe use pickle is a better idea\n",
    "        cache_key = y_collection.tobytes()\n",
    "\n",
    "        if cache_key in self.split_cache_:\n",
    "            # bad samples indices\n",
    "            D_l_sample_indices = self.split_cache_[cache_key][\"l_x_i\"]\n",
    "            # good samples indices\n",
    "            D_g_sample_indices = self.split_cache_[cache_key][\"g_x_i\"]      \n",
    "        else:\n",
    "            rank = nondominated_sort(y_collection)\n",
    "            \n",
    "        return D_l_sample_indices, D_g_sample_indices\n",
    "        \n",
    "\n",
    "    def sample(self):\n",
    "        # Load the observed hyperparameter values and corresponding function values\n",
    "        hyper_param, y_collection = self.load_hyper_param_and_y_collection()\n",
    "\n",
    "\n",
    "    \n",
    "    def load_hyper_param_and_y_collection(self):\n",
    "        # Load the hyperparameter values\n",
    "        hyper_param = []\n",
    "        y_collection = []\n",
    "\n",
    "        # TODO: this data structure can be vectorized\n",
    "        for obs in self.observations_:\n",
    "            if self.hyper_param_.name in obs['params']:\n",
    "                # TODO convert_h\n",
    "                hyper_param.append(self.convert_h(obs['params'][self.hyper_param_.name]))\n",
    "                y_collection.append(obs['f_value'].values())\n",
    "\n",
    "        jnp.asarray(hyper_param)\n",
    "        jnp.asarray(y_collection, axis = 0)\n",
    "        return hyper_param, y_collection\n",
    "    \n",
    "    # TODO: better name?\n",
    "    def convert_h(self, hyper_param_val):\n",
    "        try:\n",
    "            lower_bound, upper_bound, _ = self.get_bound_and_q()\n",
    "            if self.hyper_param_.type == 'loguniform':\n",
    "                hyper_param_val = lax.log(hyper_param_val)\n",
    "            return (hyper_param_val - lower_bound) / (upper_bound - lower_bound)\n",
    "        except NotImplementedError:\n",
    "            raise NotImplementedError('Categorical parameters do not have lower and upper options.')\n",
    "\n",
    "    def get_bound_and_q(self):\n",
    "        try:\n",
    "            if self.hyper_param_.type == 'loguniform':\n",
    "                return lax.log(self.hyper_param_.lower), lax.log(self.hyper_param_.upper), self.hyper_param_.q\n",
    "            else:\n",
    "                return self.hyper_param_.lower, self.hyper_param_.upper, self.hyper_param_.q\n",
    "        except NotImplementedError:\n",
    "            raise NotImplementedError('Categorical parameters do not have the log scale option.')\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motpe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
